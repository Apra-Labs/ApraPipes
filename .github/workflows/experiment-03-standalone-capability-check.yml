name: Experiment-03-Standalone-Capability-Check

# EXPERIMENT: Test CudaCapabilities-like pattern in isolation
# SUCCESS CRITERIA:
#   1. Simple C++ program detects CUDA availability
#   2. Returns gracefully when no GPU present
#   3. Provides detailed capability information
#   4. Thread-safe singleton pattern works

on:
  workflow_dispatch:  # Manual trigger only
  push:
    branches:
      - feature/get-rid-of-nocuda-builds
    paths:
      - '.github/workflows/experiment-03-standalone-capability-check.yml'

jobs:
  test-capability-detection:
    runs-on: ubuntu-latest

    steps:
      - name: Install CUDA Toolkit
        run: |
          echo "=== Installing CUDA Toolkit 11.8 ==="
          wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
          sudo dpkg -i cuda-keyring_1.0-1_all.deb
          sudo apt-get update
          sudo apt-get -y install cuda-toolkit-11-8
          echo "/usr/local/cuda-11.8/bin" >> $GITHUB_PATH

      - name: Create CudaCapabilities Test Program
        run: |
          cat > CudaCapabilities.h << 'EOF'
          #pragma once

          #include <vector>
          #include <string>
          #include <mutex>
          #include <iostream>

          class CudaCapabilities {
          public:
              struct DeviceInfo {
                  int deviceId;
                  std::string name;
                  int computeCapabilityMajor;
                  int computeCapabilityMinor;
                  size_t totalMemory;
              };

              static CudaCapabilities& getInstance() {
                  static CudaCapabilities instance;
                  return instance;
              }

              bool isAvailable() const { return mCudaAvailable; }
              int getDeviceCount() const { return mDeviceCount; }
              bool hasMinComputeCapability(int major, int minor) const;
              const std::vector<DeviceInfo>& getDevices() const { return mDevices; }
              void logCapabilities() const;

          private:
              CudaCapabilities();
              ~CudaCapabilities() = default;
              CudaCapabilities(const CudaCapabilities&) = delete;
              CudaCapabilities& operator=(const CudaCapabilities&) = delete;

              void detectDevices();

              bool mCudaAvailable;
              int mDeviceCount;
              std::vector<DeviceInfo> mDevices;
              static std::mutex sMutex;
          };
          EOF

          cat > CudaCapabilities.cpp << 'EOF'
          #include "CudaCapabilities.h"
          #include <cuda_runtime.h>

          std::mutex CudaCapabilities::sMutex;

          CudaCapabilities::CudaCapabilities()
              : mCudaAvailable(false), mDeviceCount(0)
          {
              detectDevices();
              logCapabilities();
          }

          void CudaCapabilities::detectDevices() {
              cudaError_t error = cudaGetDeviceCount(&mDeviceCount);

              if (error != cudaSuccess) {
                  std::cout << "[INFO] CUDA not available: " << cudaGetErrorString(error) << std::endl;
                  mCudaAvailable = false;
                  mDeviceCount = 0;
                  return;
              }

              if (mDeviceCount == 0) {
                  std::cout << "[INFO] No CUDA devices found" << std::endl;
                  mCudaAvailable = false;
                  return;
              }

              mCudaAvailable = true;

              for (int i = 0; i < mDeviceCount; i++) {
                  cudaDeviceProp prop;
                  cudaError_t err = cudaGetDeviceProperties(&prop, i);

                  if (err != cudaSuccess) {
                      std::cout << "[WARNING] Failed to get properties for device " << i << std::endl;
                      continue;
                  }

                  DeviceInfo info;
                  info.deviceId = i;
                  info.name = prop.name;
                  info.computeCapabilityMajor = prop.major;
                  info.computeCapabilityMinor = prop.minor;
                  info.totalMemory = prop.totalGlobalMem;

                  mDevices.push_back(info);
              }
          }

          bool CudaCapabilities::hasMinComputeCapability(int major, int minor) const {
              if (!mCudaAvailable) return false;

              for (const auto& device : mDevices) {
                  if (device.computeCapabilityMajor > major) return true;
                  if (device.computeCapabilityMajor == major &&
                      device.computeCapabilityMinor >= minor) return true;
              }
              return false;
          }

          void CudaCapabilities::logCapabilities() const {
              std::cout << "=== CUDA Capabilities ===" << std::endl;
              std::cout << "CUDA Available: " << (mCudaAvailable ? "YES" : "NO") << std::endl;
              std::cout << "Device Count: " << mDeviceCount << std::endl;

              for (const auto& device : mDevices) {
                  std::cout << "Device " << device.deviceId << ": " << device.name << std::endl;
                  std::cout << "  Compute Capability: " << device.computeCapabilityMajor
                           << "." << device.computeCapabilityMinor << std::endl;
                  std::cout << "  Total Memory: " << (device.totalMemory / 1024 / 1024) << " MB" << std::endl;
              }

              std::cout << "========================" << std::endl;
          }
          EOF

          cat > test_main.cpp << 'EOF'
          #include "CudaCapabilities.h"
          #include <iostream>
          #include <thread>
          #include <vector>

          // Test function for multi-threading
          void testThread(int threadId) {
              auto& cuda = CudaCapabilities::getInstance();
              std::cout << "Thread " << threadId << ": CUDA available = "
                       << cuda.isAvailable() << std::endl;
          }

          int main() {
              std::cout << "=== CudaCapabilities Standalone Test ===" << std::endl;

              // Test 1: Singleton access
              auto& cuda1 = CudaCapabilities::getInstance();
              auto& cuda2 = CudaCapabilities::getInstance();

              if (&cuda1 == &cuda2) {
                  std::cout << "✓ Singleton pattern works" << std::endl;
              } else {
                  std::cout << "✗ Singleton pattern FAILED" << std::endl;
                  return 1;
              }

              // Test 2: Basic queries
              std::cout << "\nCUDA Available: " << cuda1.isAvailable() << std::endl;
              std::cout << "Device Count: " << cuda1.getDeviceCount() << std::endl;

              // Test 3: Capability queries (should return false on GitHub runner)
              bool hasCompute52 = cuda1.hasMinComputeCapability(5, 2);
              std::cout << "Has Compute Cap >= 5.2: " << hasCompute52 << std::endl;

              // Test 4: Thread safety
              std::cout << "\nTesting thread safety..." << std::endl;
              std::vector<std::thread> threads;
              for (int i = 0; i < 5; i++) {
                  threads.emplace_back(testThread, i);
              }
              for (auto& t : threads) {
                  t.join();
              }
              std::cout << "✓ Thread safety test complete" << std::endl;

              // Test 5: Expected behavior on GitHub runner
              if (!cuda1.isAvailable()) {
                  std::cout << "\n✓ SUCCESS: Correctly detected no CUDA devices" << std::endl;
                  std::cout << "This is expected on GitHub runner (no GPU)" << std::endl;
                  return 0;
              } else {
                  std::cout << "\nUnexpected: CUDA devices found" << std::endl;
                  std::cout << "Still a success - detection works!" << std::endl;
                  return 0;
              }
          }
          EOF

      - name: Compile Test Program
        run: |
          export PATH=/usr/local/cuda-11.8/bin:$PATH
          export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH

          # Compile with static CUDA runtime
          nvcc CudaCapabilities.cpp test_main.cpp -o test_capabilities \
               -std=c++11 \
               -cudart static \
               -lcudart_static -ldl -lrt -lpthread

          echo "✓ Compilation successful"

      - name: Run Test Program
        run: |
          export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH

          ./test_capabilities

          exitCode=$?
          if [ $exitCode -eq 0 ]; then
              echo "✓ Test program ran successfully"
          else
              echo "✗ Test program failed with exit code $exitCode"
              exit 1
          fi

      - name: Test Module-Like Usage
        run: |
          cat > test_module.cpp << 'EOF'
          #include "CudaCapabilities.h"
          #include <iostream>
          #include <stdexcept>

          // Simulate a CUDA module that checks capabilities
          class ResizeNPPISimulator {
          public:
              ResizeNPPISimulator() {
                  auto& cuda = CudaCapabilities::getInstance();

                  if (!cuda.isAvailable()) {
                      throw std::runtime_error(
                          "ResizeNPPI requires CUDA device but none available. "
                          "Use ImageResizeCV for CPU-based resizing.");
                  }

                  if (!cuda.hasMinComputeCapability(3, 0)) {
                      throw std::runtime_error(
                          "NPP library requires compute capability >= 3.0");
                  }

                  std::cout << "✓ ResizeNPPI initialized successfully" << std::endl;
              }
          };

          int main() {
              std::cout << "=== Module Capability Check Test ===" << std::endl;

              try {
                  ResizeNPPISimulator module;
                  std::cout << "Unexpected: Module created on non-GPU system" << std::endl;
                  return 1;
              } catch (const std::runtime_error& e) {
                  std::cout << "✓ Caught expected exception: " << e.what() << std::endl;
                  std::cout << "✓ SUCCESS: Module correctly rejects initialization without GPU" << std::endl;
                  return 0;
              }
          }
          EOF

          export PATH=/usr/local/cuda-11.8/bin:$PATH
          export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH

          nvcc CudaCapabilities.cpp test_module.cpp -o test_module \
               -std=c++11 \
               -cudart static \
               -lcudart_static -ldl -lrt -lpthread

          ./test_module

      - name: Summary
        if: always()
        run: |
          echo ""
          echo "======================================"
          echo "EXPERIMENT 03: RESULTS"
          echo "======================================"
          echo ""
          echo "This experiment validates:"
          echo "  ✓ CudaCapabilities singleton pattern"
          echo "  ✓ Thread-safe initialization"
          echo "  ✓ Graceful handling of no GPU"
          echo "  ✓ Module-style capability checking"
          echo "  ✓ Static CUDA runtime linking"
          echo ""
          echo "Next Step: Implement in ApraPipes codebase"
          echo "======================================"
